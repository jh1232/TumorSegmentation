{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "heavy-blowing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports and constants\n",
    "import os\n",
    "import glob\n",
    "import nibabel as nib\n",
    "import nilearn\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import scipy\n",
    "import shutil\n",
    "from tensorflow import keras\n",
    "from keras import Sequential, Model\n",
    "from keras.layers import Dense, Flatten, Dropout, Reshape, Conv3D, Conv2D, Activation, UpSampling3D, \\\n",
    "                         MaxPooling3D, SpatialDropout3D, BatchNormalization, Conv3DTranspose\n",
    "from keras.layers import concatenate, Input\n",
    "# from keras.engine import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nilearn import image\n",
    "from datetime import datetime\n",
    "from scipy import ndimage\n",
    "import keras_unet\n",
    "import imageio\n",
    "\n",
    "\n",
    "###### MODIFY HPARAMS ###########\n",
    "\n",
    "DEBUG = False # for testing purposes\n",
    "\n",
    "K_FOLD_VAL_NUM = \"ALLFOLDS\"\n",
    "\n",
    "SEQUENCES = [\"t1ce\", \"flair\", \"t1\", \"t2\"]\n",
    "\n",
    "TYPE_OF_SEGMENTATION = \"ET\"\n",
    "\n",
    "##################################\n",
    "\n",
    "\n",
    "N_EPOCHS = 60 if DEBUG == False else 2\n",
    "\n",
    "\n",
    "DEBUG_SAMPLES = 20\n",
    "\n",
    "print(tf.test.is_gpu_available())\n",
    "\n",
    "# TRAINING_DATA_ROOT_DIR = \"data/\"\n",
    "# TRAIN_DIRS = [\"train/LGG\", \"train/HGG\"]\n",
    "\n",
    "TRAINING_DATA_ROOT_DIR = \"data_folds/\"\n",
    "\n",
    "TRAIN_DIRS = [\"fold_\" + str(i + 1) for i in range(5) if i + 1 != K_FOLD_VAL_NUM]\n",
    "\n",
    "print(TRAIN_DIRS)\n",
    "\n",
    "VAL_DIRS = [\"val\"]\n",
    "\n",
    "\n",
    "GROUND_TRUTH_FILE = \"seg\"\n",
    "\n",
    "\n",
    "\n",
    "N_X = 240\n",
    "N_Y = 240\n",
    "N_Z = 155\n",
    "NUM_CHANNELS = len(SEQUENCES)\n",
    "NUM_CLASSES = 4 if TYPE_OF_SEGMENTATION == \"ALL\" else 2\n",
    "\n",
    "\n",
    "PATCH_X = 80\n",
    "PATCH_Y = 80\n",
    "PATCH_Z = 80\n",
    "\n",
    "\n",
    "BATCH_SIZE = 2\n",
    "\n",
    "PERCENTILE_95 = 10.204082 * 2 # 20 pixels\n",
    "\n",
    "SEQUENCES_JOINED = \",\".join(SEQUENCES)\n",
    "\n",
    "EXPERIMENT_NAME = f\"{TYPE_OF_SEGMENTATION}_{SEQUENCES_JOINED}_{N_EPOCHS}-epochs_fold-{K_FOLD_VAL_NUM}-holdout\"\n",
    "\n",
    "if DEBUG:\n",
    "    EXPERIMENT_NAME = \"DEBUG_EXPERIMENT\"\n",
    "\n",
    "\n",
    "print(EXPERIMENT_NAME)\n",
    "\n",
    "EXPERIMENT_FOLDER = os.path.join(\"experiments\", EXPERIMENT_NAME)\n",
    "\n",
    "CKPT_PATH_PREFIX = os.path.join(EXPERIMENT_FOLDER, EXPERIMENT_NAME + \"_ckpt\")\n",
    "\n",
    "print(CKPT_PATH_PREFIX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08059ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import tensorflow.keras\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "import tensorflow as tf\n",
    "\n",
    "print(f\"Tensor Flow Version: {tf.__version__}\")\n",
    "print(f\"Keras Version: {tensorflow.keras.__version__}\")\n",
    "print()\n",
    "print(f\"Python {sys.version}\")\n",
    "print(f\"Pandas {pd.__version__}\")\n",
    "print(f\"Scikit-Learn {sk.__version__}\")\n",
    "gpu = len(tf.config.list_physical_devices('GPU'))>0\n",
    "print(\"GPU is\", \"available\" if gpu else \"NOT AVAILABLE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af600947",
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEBUG == True:\n",
    "    if os.path.isdir(EXPERIMENT_FOLDER):\n",
    "        shutil.rmtree(EXPERIMENT_FOLDER)\n",
    "\n",
    "if os.path.isdir(EXPERIMENT_FOLDER):\n",
    "    raise ValueError(f\"Experiment Folder {EXPERIMENT_FOLDER} already exists\")\n",
    "        \n",
    "        \n",
    "os.makedirs(EXPERIMENT_FOLDER, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "general-florist",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading data into X and y\n",
    "\n",
    "def read_training_data(root_path, dirs):\n",
    "    # list of numpy arrays\n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    for d in dirs:\n",
    "        path = root_path + d\n",
    "        samples = sorted(glob.glob('{}/*/'.format(path)))\n",
    "\n",
    "        if DEBUG: # don't want to train on entire dataset if debugging\n",
    "            samples = samples[:DEBUG_SAMPLES]\n",
    "        \n",
    "        for t in samples:\n",
    "            x_sample = []\n",
    "            y_sample = []\n",
    "            for s in SEQUENCES:\n",
    "                x_file = glob.glob('{}/*{}.nii.gz'.format(t, s))[0] # this is unique\n",
    "                x_sample.append(np.array(nilearn.image.get_data(x_file)))\n",
    "                \n",
    "            X.append(np.array(x_sample))\n",
    "            y_file = glob.glob('{}/*{}.nii.gz'.format(t, GROUND_TRUTH_FILE))[0] # this is unique\n",
    "            y.append(np.array(nilearn.image.get_data(y_file)))\n",
    "            \n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "\n",
    "X, y = read_training_data(TRAINING_DATA_ROOT_DIR, TRAIN_DIRS)\n",
    "\n",
    "print(\"X shape: \", X.shape)\n",
    "print(\"y shape: \", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scenic-stomach",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing\n",
    "\n",
    "def get_patches(X, y, mode):\n",
    "    \n",
    "    X_reshaped = []\n",
    "    y_reshaped = []\n",
    "    \n",
    "    for b in range(y.shape[0]):\n",
    "\n",
    "        if mode == \"Training\":\n",
    "            which_sampling = np.random.uniform(0, 1)\n",
    "            threshold = 1 # alpha value\n",
    "            \n",
    "            if np.count_nonzero(y[b, :, :, :, 1] > 0):\n",
    "                try:\n",
    "                    centre_of_mass = ndimage.measurements.center_of_mass(y[b, :, :, :, 1])\n",
    "                except:\n",
    "                    which_sampling = 1\n",
    "            else:\n",
    "                which_sampling = 1\n",
    "            \n",
    "            \n",
    "            # select which sampling mode we want\n",
    "            if which_sampling < threshold:\n",
    "                for i in range(100):\n",
    "                    x_mid = int(centre_of_mass[0] + np.random.normal(0, PERCENTILE_95))\n",
    "                    y_mid = int(centre_of_mass[1] + np.random.normal(0, PERCENTILE_95))\n",
    "                    z_mid = int(centre_of_mass[2] + np.random.normal(0, PERCENTILE_95))\n",
    "\n",
    "                    PATCH_X_RADIUS = int(PATCH_X/2)\n",
    "                    PATCH_Y_RADIUS = int(PATCH_Y/2)\n",
    "                    PATCH_Z_RADIUS = int(PATCH_Z/2)\n",
    "\n",
    "                    if i != 99: # patch is out of bounds\n",
    "                        if x_mid - PATCH_X_RADIUS < 0 or x_mid + PATCH_X_RADIUS >= N_X or y_mid - PATCH_Y_RADIUS < 0 \\\n",
    "                           or y_mid + PATCH_Y_RADIUS >= N_Y or z_mid - PATCH_Z_RADIUS < 0 or z_mid + PATCH_Z_RADIUS >= N_Z:\n",
    "                            continue\n",
    "\n",
    "                    if i == 99:\n",
    "                        print(\"Generate anyway\")\n",
    "                        # Failed to generate within bounds, generating around middle of image\n",
    "                        x_mid = int(N_X/2 + np.random.normal(0, PERCENTILE_95/4))\n",
    "                        y_mid = int(N_Y/2 + np.random.normal(0, PERCENTILE_95/4))\n",
    "                        z_mid = int(N_Z/2 + np.random.normal(0, PERCENTILE_95/4))\n",
    "                        \n",
    "\n",
    "                    X_reshaped.append(np.copy(X[b, x_mid - PATCH_X_RADIUS : x_mid + PATCH_X_RADIUS, \\\n",
    "                                                y_mid - PATCH_Y_RADIUS : y_mid + PATCH_Y_RADIUS, \\\n",
    "                                                z_mid - PATCH_Z_RADIUS : z_mid + PATCH_Z_RADIUS, :]))\n",
    "                    y_reshaped.append(np.copy(y[b, x_mid - PATCH_X_RADIUS : x_mid + PATCH_X_RADIUS, \\\n",
    "                                                y_mid - PATCH_Y_RADIUS : y_mid + PATCH_Y_RADIUS, \\\n",
    "                                                z_mid - PATCH_Z_RADIUS : z_mid + PATCH_Z_RADIUS, :]))\n",
    "\n",
    "                    break\n",
    "            else:\n",
    "                done = False\n",
    "                for retries in range(100):\n",
    "                    x_rand = np.random.randint(0, N_X - PATCH_X + 1)\n",
    "                    y_rand = np.random.randint(0, N_Y - PATCH_Y + 1)\n",
    "                    z_rand = np.random.randint(0, N_Z - PATCH_Z + 1)\n",
    "                    \n",
    "                    if (np.count_nonzero(y[b, x_rand:x_rand + PATCH_X, y_rand:y_rand + PATCH_Y, z_rand:z_rand + PATCH_Z, 1]) > 100):\n",
    "                        X_reshaped.append(np.copy(X[b, x_rand:x_rand + PATCH_X, y_rand:y_rand + PATCH_Y, z_rand:z_rand + PATCH_Z, :]))\n",
    "                        y_reshaped.append(np.copy(y[b, x_rand:x_rand + PATCH_X, y_rand:y_rand + PATCH_Y, z_rand:z_rand + PATCH_Z, :]))\n",
    "                        done = True\n",
    "                        break\n",
    "                        \n",
    "                if not done: # failed to pick a patch with at least 100 tumorous voxels. Generate anyway.\n",
    "                    x_rand = np.random.randint(0, N_X - PATCH_X + 1)\n",
    "                    y_rand = np.random.randint(0, N_Y - PATCH_Y + 1)\n",
    "                    z_rand = np.random.randint(0, N_Z - PATCH_Z + 1)\n",
    "                    \n",
    "                    X_reshaped.append(np.copy(X[b, x_rand:x_rand + PATCH_X, y_rand:y_rand + PATCH_Y, z_rand:z_rand + PATCH_Z, :]))\n",
    "                    y_reshaped.append(np.copy(y[b, x_rand:x_rand + PATCH_X, y_rand:y_rand + PATCH_Y, z_rand:z_rand + PATCH_Z, :]))\n",
    "\n",
    "\n",
    "        elif mode == \"Validation\":\n",
    "            x_rand = np.random.randint(0, N_X - PATCH_X + 1)\n",
    "            y_rand = np.random.randint(0, N_Y - PATCH_Y + 1)\n",
    "            z_rand = np.random.randint(0, N_Z - PATCH_Z + 1)\n",
    "\n",
    "            X_reshaped.append(np.copy(X[b, x_rand:x_rand + PATCH_X, y_rand:y_rand + PATCH_Y, z_rand:z_rand + PATCH_Z, :]))\n",
    "            y_reshaped.append(np.copy(y[b, x_rand:x_rand + PATCH_X, y_rand:y_rand + PATCH_Y, z_rand:z_rand + PATCH_Z, :]))\n",
    "                    \n",
    "    X_reshaped = np.array(X_reshaped)\n",
    "    y_reshaped = np.array(y_reshaped)\n",
    "    \n",
    "   \n",
    "    return X_reshaped, y_reshaped\n",
    "\n",
    "\n",
    "\n",
    "def _standardize_nii_file(x):\n",
    "    ret = np.copy(x)\n",
    "    ret = (ret - x.mean())/x.std() if x.std() > 0 else (ret - x.mean())\n",
    "    return ret\n",
    "\n",
    "def standardize_training_features(X, y):\n",
    "    for i in range(X.shape[0]):\n",
    "        X[i, :, :, :, :] = np.array(_standardize_nii_file(X[i, :, :, :, :]))\n",
    "    return X, y\n",
    "\n",
    "def _convert_labels(y):\n",
    "    # This section of code modifies the labels in the training dataset.\n",
    "    # The BraTS dataset provides 4 different labels (0, 1, 2, 4) on a pixel level\n",
    "    # But we don't care about this level of detail. We only want to identify whether\n",
    "    # a pixel is tumorous (0/1 classification).\n",
    "    # In particular, we only care if it is an \"enhancing tumor structure\"\n",
    "    # This is the light blue section here: https://www.med.upenn.edu/sbia/brats2018.html\n",
    "    # From here (https://arxiv.org/pdf/1811.02629.pdf), we see that the labels correspond to the following\n",
    "    # For BraTS 2017 and above (note Label 3 has been combined with Label 1):\n",
    "    # Label 1 (+ 3): NCR -- necrotic core, and NET -- Non enhancing tumor\n",
    "    # Label 2: ED -- Edema\n",
    "    # Label 4: AT -- Enhancing regions within the gross tumor abnormality\n",
    "    # Thus, we only care about Label 4. We should therefore set labels 1 and 2 to label 0, and then\n",
    "    # set label 4 to 1 to achieve what we want.\n",
    "    \n",
    "    # Note: 3 is not present in this dataset\n",
    "    \n",
    "    # Update: to segment TC, set 2 -> 0, 4 -> 1. \n",
    "    # to segment ET, set 1->0, 2->0, 4->1\n",
    "    \n",
    "    \n",
    "    if TYPE_OF_SEGMENTATION == \"TC\":\n",
    "        y[y == 2] = 0\n",
    "        y[y == 4] = 1\n",
    "    elif TYPE_OF_SEGMENTATION == \"ET\":\n",
    "        y[y == 1] = 0\n",
    "        y[y == 2] = 0\n",
    "        y[y == 4] = 1\n",
    "    elif TYPE_OF_SEGMENTATION == \"ALL\":\n",
    "        y[y == 4] = 3\n",
    "    else:\n",
    "        raise ValueError('invalid segmentation type.')\n",
    "        \n",
    "        \n",
    "    return y\n",
    "\n",
    "\n",
    "def convert_target_labels(X, y):\n",
    "    y = _convert_labels(y)\n",
    "    return X, y\n",
    "\n",
    "def switch_to_last_channel_mode(X, y):\n",
    "    X = np.moveaxis(X, [0, 1, 2, 3, 4], [0, 4, 1, 2, 3])\n",
    "    return X, y\n",
    "\n",
    "def one_hot_labels(X, y):\n",
    "    y = keras.utils.to_categorical(y, NUM_CLASSES)\n",
    "    return X, y\n",
    "\n",
    "\n",
    "\n",
    "def preprocess_data(X, y, validation=False):\n",
    "    X, y = convert_target_labels(X, y)\n",
    "    \n",
    "    #X, y = one_hot_labels(X, y)\n",
    "    \n",
    "    X, y = switch_to_last_channel_mode(X, y)\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def translation(data):\n",
    "    return scipy.ndimage.interpolation.shift(data, (5, 5, 0, 0), order=0, mode='nearest')\n",
    "\n",
    "def rotation(data):\n",
    "    rotation_angles = [0, 90, 180, 270]\n",
    "    angle = rotation_angles[np.random.randint(4)]\n",
    "    \n",
    "    return scipy.ndimage.rotate(data, angle, reshape=False, order=0, mode='nearest')\n",
    "\n",
    "def intensity(data):\n",
    "    return data*float(random.uniform(0.8, 1.2))\n",
    "\n",
    "def flip(data):\n",
    "    if random.uniform(0, 1) < 0.5:\n",
    "        data = np.fliplr(data)\n",
    "    if random.uniform(0, 1) < 0.5:\n",
    "        data = np.flipud(data)\n",
    "    return data\n",
    "\n",
    "def data_augmentation(data):\n",
    "    # https://mlnotebook.github.io/post/dataaug/\n",
    "    ret = np.copy(data)\n",
    "    \n",
    "    # disabling for now\n",
    "    \"\"\"\n",
    "    if random.uniform(0, 1) < 0.25: # randomly augment the data.\n",
    "        if random.uniform(0, 1) < 0.25:\n",
    "            ret = translation(ret)\n",
    "\n",
    "        #if random.uniform(0, 1) < 0.20:\n",
    "        #    ret = intensity(ret)\n",
    "\n",
    "        if random.uniform(0, 1) < 0.25:\n",
    "            ret = rotation(ret)\n",
    "\n",
    "        if random.uniform(0, 1) < 0.25:\n",
    "            ret = flip(ret)\n",
    "    \"\"\"\n",
    "    return ret\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def read_training_data_from_memory(data_ids):\n",
    "    X_train = []\n",
    "    y_train = []\n",
    "    for i in data_ids:\n",
    "        X_train.append(data_augmentation(X[i, :, :, :, :]))\n",
    "        y_train.append(data_augmentation(y[i, :, :, :]))\n",
    "    return one_hot_labels(np.array(X_train), np.array(y_train))\n",
    "\n",
    "def read_validation_data_from_memory(data_ids):\n",
    "    X_train = []\n",
    "    y_train = []\n",
    "    for i in data_ids:\n",
    "        X_train.append(data_augmentation(X_val[i, :, :, :, :]))\n",
    "        y_train.append(data_augmentation(y_val[i, :, :, :]))\n",
    "    return one_hot_labels(np.array(X_train), np.array(y_train))\n",
    "\n",
    "\n",
    "def get_data(data_ids, mode):\n",
    "    \n",
    "    if mode == \"Training\":\n",
    "        X, y = read_training_data_from_memory(data_ids)\n",
    "    elif mode == \"Validation\":\n",
    "        X, y = read_validation_data_from_memory(data_ids)\n",
    "    \n",
    "    X, y = get_patches(X, y, mode)\n",
    "    \n",
    "    #X, y = standardize_training_features(X, y)\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "\n",
    "X, y = standardize_training_features(X, y)\n",
    "\n",
    "X, X_test, y, y_test = train_test_split(X, y, test_size=0.1)\n",
    "\n",
    "X, X_val, y, y_val = train_test_split(X, y, test_size=0.1)\n",
    "\n",
    "X, y = preprocess_data(X, y)\n",
    "\n",
    "X_val, y_val = preprocess_data(X_val, y_val, validation=True)\n",
    "\n",
    "X_test, y_test = preprocess_data(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "X_val = None\n",
    "X_test = None\n",
    "y_val = None\n",
    "y_test = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wicked-bouquet",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stanford.edu/~shervine/blog/keras-how-to-generate-data-on-the-fly\n",
    "\n",
    "class DataGenerator(keras.utils.Sequence):\n",
    "    # 'Generates data for Keras'\n",
    "    def __init__(self, size, batch_size=BATCH_SIZE, dim=(PATCH_X,PATCH_Y,PATCH_Z), n_channels=NUM_CHANNELS,\n",
    "                 n_classes=NUM_CLASSES, shuffle=True, mode=\"Training\"):\n",
    "        'Initialization'\n",
    "        self.dim = dim\n",
    "        self.batch_size = batch_size\n",
    "        self.size = size\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.shuffle = shuffle\n",
    "        self.mode = mode\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        \n",
    "        N = 3000 if self.mode == \"Training\" else 300\n",
    "        \n",
    "        return int(N/self.batch_size)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        indexes = []\n",
    "        for i in range(self.batch_size):\n",
    "            indexes.append(random.randint(0, self.size - 1))\n",
    "\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(indexes)\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        pass\n",
    "\n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        'Generates data containing batch_size samples'\n",
    "\n",
    "        X, y = get_data(list_IDs_temp, self.mode)\n",
    "        \n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "perfect-seattle",
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics\n",
    "\n",
    "def recall(y, y_hat):\n",
    "    true_positives = K.sum(K.round(K.clip(y * y_hat, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y, 0, 1)))\n",
    "    return true_positives/(possible_positives + K.epsilon())\n",
    "\n",
    "def precision(y, y_hat):\n",
    "    true_positives = K.sum(K.round(K.clip(y * y_hat, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_hat, 0, 1)))\n",
    "    return true_positives/(predicted_positives + K.epsilon())\n",
    "\n",
    "def f1(y, y_hat):\n",
    "    r = recall(y, y_hat)\n",
    "    p = precision(y, y_hat)\n",
    "    return 2*((p*r)/(p + r + K.epsilon()))\n",
    "\n",
    "\n",
    "def dice_coef(y_true, y_pred, smooth=1):\n",
    "    intersection = K.sum(K.abs(y_true * y_pred), axis=(0, 1, 2, 3))\n",
    "    dice_numerator = (2. * intersection + smooth)\n",
    "    dice_denominator = (K.sum(K.square(y_true),axis=(0, 1, 2, 3)) + K.sum(K.square(y_pred),axis=(0, 1, 2, 3)) + smooth)\n",
    "\n",
    "    return dice_numerator/dice_denominator\n",
    "\n",
    "\n",
    "def dice_coef_loss(y_true, y_pred):\n",
    "    return 1- K.mean(dice_coef(y_true, y_pred))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def tversky(y_true, y_pred, smooth=0.01):\n",
    "    true_pos = K.sum(y_true * y_pred, axis=(0, 1, 2, 3))\n",
    "    false_neg = K.sum(y_true * (1-y_pred), axis=(0, 1, 2, 3))\n",
    "    false_pos = K.sum((1-y_true)*y_pred, axis=(0, 1, 2, 3))\n",
    "    alpha = 0.7\n",
    "    return (true_pos + smooth)/(true_pos + alpha*false_neg + (1-alpha)*false_pos + smooth)\n",
    "\n",
    "def tversky_loss(y_true, y_pred):\n",
    "    return 1 - K.mean(tversky(y_true,y_pred))\n",
    "\n",
    "\n",
    "def combined_loss(y_true, y_pred):\n",
    "    return 0.5*dice_coef_loss(y_true, y_pred) + 0.5*tversky_loss(y_true, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "transsexual-seller",
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline model\n",
    "def unet_2d_model():\n",
    "    input_shape = (240, 240, NUM_CLASSES)\n",
    "    model = keras_unet.models.custom_unet(input_shape, use_batch_norm=True, num_classes=NUM_CLASSES, filters=64, dropout=0.5, output_activation='softmax')\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "def basic_cnn_model():\n",
    "    \n",
    "    # our 3D U-Net model\n",
    "    \n",
    "    inputs = Input(shape=(None, None, None, NUM_CHANNELS))\n",
    "    \n",
    "    NUM_FILTERS = 32\n",
    "    \n",
    "    DROPOUT_PROB = 0.5\n",
    "    \n",
    "    kernel = (3, 3, 3)\n",
    "    \n",
    "    stride = (2, 2, 2)\n",
    "    \n",
    "    pooling = (2, 2, 2)\n",
    "    \n",
    "    #contraction\n",
    "    \n",
    "    X = Conv3D(filters=NUM_FILTERS, kernel_size=kernel, padding='same', activation='relu')(inputs)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = SpatialDropout3D(DROPOUT_PROB)(X)\n",
    "    X = Conv3D(filters=NUM_FILTERS, kernel_size=kernel, padding='same', activation='relu')(X)\n",
    "    X = BatchNormalization()(X)\n",
    "    concat_1 = X\n",
    "    X = MaxPooling3D(pool_size=pooling)(X)\n",
    "    \n",
    "    X = Conv3D(filters=NUM_FILTERS * 2, kernel_size=kernel, padding='same', activation='relu')(X)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = SpatialDropout3D(DROPOUT_PROB)(X)\n",
    "    X = Conv3D(filters=NUM_FILTERS * 2, kernel_size=kernel, padding='same', activation='relu')(X)\n",
    "    X = BatchNormalization()(X)\n",
    "    concat_2 = X\n",
    "    X = MaxPooling3D(pool_size=pooling)(X)\n",
    "    \n",
    "    X = Conv3D(filters=NUM_FILTERS * 4, kernel_size=kernel, padding='same', activation='relu')(X)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = SpatialDropout3D(DROPOUT_PROB)(X)\n",
    "    X = Conv3D(filters=NUM_FILTERS * 4, kernel_size=kernel, padding='same', activation='relu')(X)\n",
    "    X = BatchNormalization()(X)\n",
    "    concat_3 = X\n",
    "    X = MaxPooling3D(pool_size=pooling)(X)\n",
    "    \n",
    "    X = Conv3D(filters=NUM_FILTERS * 8, kernel_size=kernel, padding='same', activation='relu')(X)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = SpatialDropout3D(DROPOUT_PROB)(X)\n",
    "    X = Conv3D(filters=NUM_FILTERS * 8, kernel_size=kernel, padding='same', activation='relu')(X)\n",
    "    X = BatchNormalization()(X)\n",
    "    concat_4 = X\n",
    "    X = MaxPooling3D(pool_size=pooling)(X)\n",
    "    \n",
    "    # trough\n",
    "    \n",
    "    X = Conv3D(filters=NUM_FILTERS * 16, kernel_size=kernel, padding='same', activation='relu')(X)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = SpatialDropout3D(DROPOUT_PROB)(X)\n",
    "    X = Conv3D(filters=NUM_FILTERS * 16, kernel_size=kernel, padding='same', activation='relu')(X)\n",
    "    X = BatchNormalization()(X)\n",
    "    \n",
    "    # expansion\n",
    "    \n",
    "    X = Conv3DTranspose(NUM_FILTERS * 8, pooling, strides=stride, padding='same')(X)\n",
    "    X = concatenate([X, concat_4], axis=4)\n",
    "    X = Conv3D(filters=NUM_FILTERS * 8, kernel_size=kernel, padding='same', activation='relu')(X)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = SpatialDropout3D(DROPOUT_PROB)(X)\n",
    "    X = Conv3D(filters=NUM_FILTERS * 8, kernel_size=kernel, padding='same', activation='relu')(X)\n",
    "    X = BatchNormalization()(X)\n",
    "    \n",
    "    X = Conv3DTranspose(NUM_FILTERS * 4, pooling, strides=stride, padding='same')(X)\n",
    "    X = concatenate([X, concat_3], axis=4)\n",
    "    X = Conv3D(filters=NUM_FILTERS * 4, kernel_size=kernel, padding='same', activation='relu')(X)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = SpatialDropout3D(DROPOUT_PROB)(X)\n",
    "    X = Conv3D(filters=NUM_FILTERS * 4, kernel_size=kernel, padding='same', activation='relu')(X)\n",
    "    X = BatchNormalization()(X)\n",
    "    \n",
    "    X = Conv3DTranspose(NUM_FILTERS * 2, pooling, strides=stride, padding='same')(X)\n",
    "    X = concatenate([X, concat_2], axis=4)\n",
    "    X = Conv3D(filters=NUM_FILTERS * 2, kernel_size=kernel, padding='same', activation='relu')(X)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = SpatialDropout3D(DROPOUT_PROB)(X)\n",
    "    X = Conv3D(filters=NUM_FILTERS * 2, kernel_size=kernel, padding='same', activation='relu')(X)\n",
    "    X = BatchNormalization()(X)\n",
    "    \n",
    "    X = Conv3DTranspose(NUM_FILTERS, pooling, strides=stride, padding='same')(X)\n",
    "    X = concatenate([X, concat_1], axis=4)\n",
    "    X = Conv3D(filters=NUM_FILTERS, kernel_size=kernel, padding='same', activation='relu')(X)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = SpatialDropout3D(DROPOUT_PROB)(X)\n",
    "    X = Conv3D(filters=NUM_FILTERS, kernel_size=kernel, padding='same', activation='relu')(X)\n",
    "    X = BatchNormalization()(X)\n",
    "    \n",
    "    # segmentation\n",
    "    \n",
    "    outputs = Conv3D(filters=NUM_CLASSES, kernel_size=1, padding='same', activation='softmax')(X)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indoor-contractor",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_unet.metrics import iou, iou_thresholded\n",
    "from keras.callbacks import ModelCheckpoint,  LearningRateScheduler\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.metrics import MeanIoU\n",
    "import math\n",
    "\n",
    "# training\n",
    "\n",
    "#model = unet_2d_model()\n",
    "model = basic_cnn_model()\n",
    "\n",
    "model.compile(optimizer=Adam(lr=0.0001),\n",
    "              loss=dice_coef_loss,\n",
    "              metrics=[f1]\n",
    "             )\n",
    "\n",
    "training_generator = DataGenerator(X.shape[0], mode=\"Training\")\n",
    "#validation_generator = DataGenerator(X_val.shape[0], mode=\"Validation\")\n",
    "\n",
    "training_history = model.fit(x=training_generator, #validation_data=validation_generator, \n",
    "                             epochs=N_EPOCHS, verbose=1,\n",
    "                             callbacks=[ModelCheckpoint(filepath=CKPT_PATH_PREFIX + \".{epoch:02d}.h5\",  save_freq='epoch', period=5)])\n",
    "\n",
    "model.save(CKPT_PATH_PREFIX + \".h5\") \n",
    "\n",
    "\n",
    "model.summary()\n",
    "\n",
    "\n",
    "\n",
    "#model.save(\"HGG_model_verification.h5\") \n",
    "\n",
    "\n",
    "#model = keras.models.load_model('HGG_model_original_alpha1.h5', custom_objects={'dice_coef_loss': dice_coef_loss, 'f1': f1})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70b34fd",
   "metadata": {},
   "source": [
    "# Statistics Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc8bdd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#validation inference\n",
    "\n",
    "CHECKPOINTS = {\n",
    "    \"TC_flair_60-epochs_fold-1-holdout\": \"TC\",\n",
    "    \"TC_t1ce_60-epochs_fold-1-holdout\": \"TC\",\n",
    "}\n",
    "\n",
    "SEQUENCES_LIST = {\n",
    "    \"TC_t1ce_60-epochs_fold-1-holdout\": [\"t1ce\"],\n",
    "    \"TC_flair_60-epochs_fold-1-holdout\": [\"flair\"],\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def read_validation_data(root_path, dirs, seq_list):\n",
    "    # list of numpy arrays\n",
    "    X = []\n",
    "    sample_names = []\n",
    "    \n",
    "    for d in dirs:\n",
    "        path = root_path + d\n",
    "        samples = sorted(glob.glob('{}/*/'.format(path)))\n",
    "\n",
    "        for t in samples:\n",
    "            x_sample = []\n",
    "            y_sample = []\n",
    "            for s in seq_list:\n",
    "                x_file = glob.glob('{}/*{}.nii.gz'.format(t, s))[0] # this is unique\n",
    "                x_sample.append(np.array(nilearn.image.get_data(x_file)))\n",
    "                \n",
    "            X.append(np.array(x_sample))\n",
    "            sample_names.append(t)\n",
    "    X = np.array(X)\n",
    "    print(X.shape)\n",
    "    X = np.moveaxis(X, [0, 1, 2, 3, 4], [0, 4, 1, 2, 3])\n",
    "    print(X.shape)\n",
    "    return sample_names, X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d41e7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_folder = \"experiments\"\n",
    "\n",
    "def do_pred_with_ckpt(exp_name, c, segType, seq_list, train_folders):\n",
    "    model = keras.models.load_model(c, custom_objects={'dice_coef_loss': dice_coef_loss, 'f1': f1})\n",
    "    X = None\n",
    "    y = None\n",
    "#     X_validation = read_validation_data(TRAINING_DATA_ROOT_DIR, VAL_DIRS, seq_list)\n",
    "    \n",
    "    samples, X_validation = read_validation_data(TRAINING_DATA_ROOT_DIR, train_folders, seq_list)\n",
    "\n",
    "    print(\"X_validation shape: \", X_validation.shape)\n",
    "    #print(\"y shape: \", y.shape)\n",
    "    \n",
    "    NUM_CLASSES = 4 if segType == \"ALL\" else 2\n",
    "    \n",
    "    NUM_CHANNELS = len(seq_list)\n",
    "    \n",
    "    #print(X_validation.shape)\n",
    "    preds = np.zeros((X_validation.shape[0], X_validation.shape[1], X_validation.shape[2], X_validation.shape[3] + 5, NUM_CLASSES))\n",
    "\n",
    "    tmp111 = np.zeros((X_validation.shape[0], X_validation.shape[1], X_validation.shape[2], X_validation.shape[3] + 5, NUM_CHANNELS))\n",
    "\n",
    "    tmp111[:, :, :, :-5, :] = X_validation\n",
    "\n",
    "    X_validation = tmp111\n",
    "\n",
    "    #print(X_validation.shape)\n",
    "\n",
    "    for b in range(0, X_validation.shape[0]):\n",
    "        #print(b)\n",
    "        X_normalized, _ = standardize_training_features(np.copy(X_validation[b:b+1, :, :, :, :]), None)\n",
    "        logits = model.predict(X_normalized, batch_size=1)\n",
    "        logits = np.squeeze(logits)\n",
    "        preds[b, :, :, :, :] += logits\n",
    "\n",
    "\n",
    "    tmp111 = None\n",
    "    X_validation = None\n",
    "\n",
    "    #print(preds.shape)\n",
    "\n",
    "    preds = preds[:, :, :, :-5, :]\n",
    "\n",
    "    #print(preds.shape)\n",
    "\n",
    "    preds = np.argmax(preds, axis=-1)\n",
    "    \n",
    "    if segType == \"TC\":\n",
    "        pass\n",
    "    elif segType == \"ET\":\n",
    "        preds[preds == 1] = 4\n",
    "    elif segType == \"ALL\":\n",
    "        preds[preds == 3] = 4\n",
    "    else:\n",
    "        raise ValueError('invalid segmentation type.')\n",
    "\n",
    "    for b in range(preds.shape[0]):\n",
    "        if np.count_nonzero(preds[b, :, :, :]) == 0:\n",
    "            #print(\"all 0\")\n",
    "            preds[b, 0, 0, 0] = 1 # to avoid invalid data errors when submitting\n",
    "    \n",
    "    chkpt_folder_name = c[:-3].replace(\".\", \"_\").split(\"/\")[-1]\n",
    "    \n",
    "    #PREDS_DIR = \"loop_preds/predictions5/\" \n",
    "    path = os.path.join(root_folder, exp_name, \"segmentations\", chkpt_folder_name)\n",
    "    \n",
    "    #samples = sorted(glob.glob('{}/*/'.format(TRAINING_DATA_ROOT_DIR + VAL_DIRS[0])))\n",
    "    #samples1 = sorted(glob.glob('{}/*/'.format(TRAINING_DATA_ROOT_DIR + TRAIN_DIRS[0])))\n",
    "    #samples2 = sorted(glob.glob('{}/*/'.format(TRAINING_DATA_ROOT_DIR + TRAIN_DIRS[1])))\n",
    "    #samples = samples1 + samples2\n",
    "    #samples = sorted(samples)\n",
    "    #samples = [x.split('/')[-2] for x in samples ]\n",
    "\n",
    "    #print(\"Outputting\", c, \"to folder\", path)\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "    #print(samples)\n",
    "\n",
    "    for i in range(preds.shape[0]):\n",
    "        s = samples[i]\n",
    "        s = s.split(\"/\")[-2]\n",
    "        output = preds[i, :, :, :]\n",
    "        output = output.astype(np.int32)\n",
    "        filename = os.path.join(path, \"{}.nii.gz\".format(s))\n",
    "        ni_img = nib.Nifti1Image(output, affine=np.eye(4))\n",
    "        nib.save(ni_img, filename)\n",
    "        #print(s, np.count_nonzero(preds[i, :, :, :]))\n",
    "\n",
    "\n",
    "\n",
    "def do_inference(f, segType):\n",
    "    chkpt_folder = os.path.join(root_folder, f)\n",
    "    # TMP JUST DO 30 EPOCHS\n",
    "    chkpts = [cp for cp in glob.glob(f\"{chkpt_folder}/*.h5\") if cp.startswith(chkpt_folder + \"/\" + f) and cp != chkpt_folder + \"/\" + f + \".h5\" and \".30.h5\" in cp]\n",
    "    #chkpts = [cp for cp in glob.glob(f\"{chkpt_folder}/*.h5\") if cp.startswith(chkpt_folder + \"/\" + f) and cp != chkpt_folder + \"/\" + f + \".h5\"]\n",
    "#     print(chkpts)\n",
    "    for c in chkpts:\n",
    "        print(c)\n",
    "        # TODO: filter out the fold used for holdout\n",
    "        for i in range(5):\n",
    "            do_pred_with_ckpt(f, c, segType, SEQUENCES_LIST[f], [\"fold_\" + str(i + 1)])\n",
    "\n",
    "\n",
    "for k, v in CHECKPOINTS.items():\n",
    "    do_inference(k, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spectacular-surveillance",
   "metadata": {},
   "source": [
    "# LOOP INFERENCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bored-treasurer",
   "metadata": {},
   "outputs": [],
   "source": [
    "#validation inference\n",
    "\n",
    "CHECKPOINTS = {\n",
    "    #\"tc_t1ce_60\": \"TC\",\n",
    "    #\"tc_flair_60\": \"TC\",\n",
    "    #\"at_t1ce_60\": \"ET\",\n",
    "    #\"et_all_100\": \"ET\", #not working....\n",
    "#     \"at_flair_60\": \"ET\"\n",
    "    #\"et_t1ce_flair_60\": \"ET\",\n",
    "    #\"tc_t1ce_flair_60\": \"TC\",\n",
    "    \"tc_all_60\": \"TC\",\n",
    "    \"et_all_60\": \"ET\"\n",
    "}\n",
    "\n",
    "SEQUENCES_LIST = {\n",
    "    #\"tc_t1ce_60\": [\"t1ce\"],\n",
    "    #\"tc_flair_60\": [\"flair\"],\n",
    "    #\"at_t1ce_60\": [\"t1ce\"],\n",
    "    #\"et_all_100\": [\"t1ce\", \"flair\", \"t2\", \"t1\"],\n",
    "#     \"at_flair_60\": [\"flair\"]\n",
    "    #\"et_t1ce_flair_60\": [\"flair\", \"t1ce\"],\n",
    "    #\"tc_t1ce_flair_60\": [\"flair\", \"t1ce\"],\n",
    "    \"tc_all_60\": [\"flair\", \"t1ce\", \"t1\", \"t2\"],\n",
    "    \"et_all_60\": [\"flair\", \"t1ce\", \"t1\", \"t2\"]\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def read_validation_data(root_path, dirs, seq_list):\n",
    "    # list of numpy arrays\n",
    "    X = []\n",
    "    \n",
    "    for d in dirs:\n",
    "        path = root_path + d\n",
    "        samples = sorted(glob.glob('{}/*/'.format(path)))\n",
    "\n",
    "        for t in samples:\n",
    "            x_sample = []\n",
    "            y_sample = []\n",
    "            for s in seq_list:\n",
    "                x_file = glob.glob('{}/*{}.nii.gz'.format(t, s))[0] # this is unique\n",
    "                x_sample.append(np.array(nilearn.image.get_data(x_file)))\n",
    "                \n",
    "            X.append(np.array(x_sample))\n",
    "    X = np.array(X)\n",
    "    print(X.shape)\n",
    "    X = np.moveaxis(X, [0, 1, 2, 3, 4], [0, 4, 1, 2, 3])\n",
    "    print(X.shape)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pressing-parent",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_folder = \"loop_preds\"\n",
    "\n",
    "def do_pred_with_ckpt(c, segType, seq_list):\n",
    "    model = keras.models.load_model(c, custom_objects={'dice_coef_loss': dice_coef_loss, 'f1': f1})\n",
    "    X = None\n",
    "    y = None\n",
    "    X_validation = read_validation_data(TRAINING_DATA_ROOT_DIR, VAL_DIRS, seq_list)\n",
    "    \n",
    "    NUM_CLASSES = 4 if segType == \"ALL\" else 2\n",
    "    \n",
    "    NUM_CHANNELS = len(seq_list)\n",
    "    \n",
    "    #print(X_validation.shape)\n",
    "    preds = np.zeros((X_validation.shape[0], X_validation.shape[1], X_validation.shape[2], X_validation.shape[3] + 5, NUM_CLASSES))\n",
    "\n",
    "    tmp111 = np.zeros((X_validation.shape[0], X_validation.shape[1], X_validation.shape[2], X_validation.shape[3] + 5, NUM_CHANNELS))\n",
    "\n",
    "    tmp111[:, :, :, :-5, :] = X_validation\n",
    "\n",
    "    X_validation = tmp111\n",
    "\n",
    "    #print(X_validation.shape)\n",
    "\n",
    "    for b in range(0, X_validation.shape[0]):\n",
    "        #print(b)\n",
    "        X_normalized, _ = standardize_training_features(np.copy(X_validation[b:b+1, :, :, :, :]), None)\n",
    "        logits = model.predict(X_normalized, batch_size=1)\n",
    "        logits = np.squeeze(logits)\n",
    "        preds[b, :, :, :, :] += logits\n",
    "\n",
    "\n",
    "    tmp111 = None\n",
    "    X_validation = None\n",
    "\n",
    "    #print(preds.shape)\n",
    "\n",
    "    preds = preds[:, :, :, :-5, :]\n",
    "\n",
    "    #print(preds.shape)\n",
    "\n",
    "    preds = np.argmax(preds, axis=-1)\n",
    "    \n",
    "    if segType == \"TC\":\n",
    "        pass\n",
    "    elif segType == \"ET\":\n",
    "        preds[preds == 1] = 4\n",
    "    elif segType == \"ALL\":\n",
    "        preds[preds == 3] = 4\n",
    "    else:\n",
    "        raise ValueError('invalid segmentation type.')\n",
    "\n",
    "    for b in range(preds.shape[0]):\n",
    "        if np.count_nonzero(preds[b, :, :, :]) == 0:\n",
    "            #print(\"all 0\")\n",
    "            preds[b, 0, 0, 0] = 1 # to avoid invalid data errors when submitting\n",
    "    \n",
    "    chkpt_folder_name = c[:-3].replace(\".\", \"_\")\n",
    "    \n",
    "    PREDS_DIR = \"loop_preds/predictions5/\" \n",
    "    path = os.path.join(PREDS_DIR, chkpt_folder_name)\n",
    "    \n",
    "    samples = sorted(glob.glob('{}/*/'.format(TRAINING_DATA_ROOT_DIR + VAL_DIRS[0])))\n",
    "    #samples1 = sorted(glob.glob('{}/*/'.format(TRAINING_DATA_ROOT_DIR + TRAIN_DIRS[0])))\n",
    "    #samples2 = sorted(glob.glob('{}/*/'.format(TRAINING_DATA_ROOT_DIR + TRAIN_DIRS[1])))\n",
    "    #samples = samples1 + samples2\n",
    "    #samples = sorted(samples)\n",
    "    samples = [x.split('/')[-2] for x in samples ]\n",
    "\n",
    "    #print(\"Outputting\", c, \"to folder\", path)\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "    #print(samples)\n",
    "\n",
    "    for i in range(preds.shape[0]):\n",
    "        s = samples[i]\n",
    "        output = preds[i, :, :, :]\n",
    "        filename = os.path.join(path, \"{}.nii.gz\".format(s))\n",
    "        ni_img = nib.Nifti1Image(output, affine=np.eye(4))\n",
    "        nib.save(ni_img, filename)\n",
    "        #print(s, np.count_nonzero(preds[i, :, :, :]))\n",
    "\n",
    "\n",
    "\n",
    "def do_inference(f, segType):\n",
    "    chkpts = [cp for cp in glob.glob(\"*.h5\") if cp.startswith(f) and cp != f + \".h5\"]\n",
    "    for c in chkpts:\n",
    "        print(c)\n",
    "        do_pred_with_ckpt(c, segType, SEQUENCES_LIST[f])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for k, v in CHECKPOINTS.items():\n",
    "    do_inference(k, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "final-stationery",
   "metadata": {},
   "source": [
    "# EVERYTHING AFTER IS THE SAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cubic-choice",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model('et_all_100.60.h5', custom_objects={'dice_coef_loss': dice_coef_loss, 'f1': f1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sunrise-curve",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = None\n",
    "y = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collaborative-lincoln",
   "metadata": {},
   "outputs": [],
   "source": [
    "#validation inference\n",
    "\n",
    "def read_validation_data(root_path, dirs):\n",
    "    # list of numpy arrays\n",
    "    X = []\n",
    "    \n",
    "    for d in dirs:\n",
    "        path = root_path + d\n",
    "        samples = sorted(glob.glob('{}/*/'.format(path)))\n",
    "\n",
    "        for t in samples:\n",
    "            x_sample = []\n",
    "            y_sample = []\n",
    "            for s in SEQUENCES:\n",
    "                x_file = glob.glob('{}/*{}.nii.gz'.format(t, s))[0] # this is unique\n",
    "                x_sample.append(np.array(nilearn.image.get_data(x_file)))\n",
    "                \n",
    "            X.append(np.array(x_sample))\n",
    "    X = np.array(X)\n",
    "    print(X.shape)\n",
    "    X = np.moveaxis(X, [0, 1, 2, 3, 4], [0, 4, 1, 2, 3])\n",
    "    print(X.shape)\n",
    "    return X\n",
    "\n",
    "\n",
    "X_validation = read_validation_data(TRAINING_DATA_ROOT_DIR, VAL_DIRS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dutch-tonight",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_validation.shape)\n",
    "preds = np.zeros((X_validation.shape[0], X_validation.shape[1], X_validation.shape[2], X_validation.shape[3] + 5, NUM_CLASSES))\n",
    "\n",
    "tmp111 = np.zeros((X_validation.shape[0], X_validation.shape[1], X_validation.shape[2], X_validation.shape[3] + 5, NUM_CHANNELS))\n",
    "\n",
    "tmp111[:, :, :, :-5, :] = X_validation\n",
    "\n",
    "X_validation = tmp111\n",
    "\n",
    "print(X_validation.shape)\n",
    "\n",
    "for b in range(0, X_validation.shape[0]):\n",
    "    print(b)\n",
    "    X_normalized, _ = standardize_training_features(np.copy(X_validation[b:b+1, :, :, :, :]), None)\n",
    "    logits = model.predict(X_normalized, batch_size=1)\n",
    "    logits = np.squeeze(logits)\n",
    "    preds[b, :, :, :, :] += logits\n",
    "     \n",
    "\n",
    "tmp111 = None\n",
    "X_validation = None\n",
    "        \n",
    "print(preds.shape)\n",
    "\n",
    "preds = preds[:, :, :, :-5, :]\n",
    "\n",
    "print(preds.shape)\n",
    "\n",
    "preds = np.argmax(preds, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reliable-boutique",
   "metadata": {},
   "outputs": [],
   "source": [
    "if TYPE_OF_SEGMENTATION == \"TC\":\n",
    "    pass\n",
    "elif TYPE_OF_SEGMENTATION == \"ET\":\n",
    "    preds[preds == 1] = 4\n",
    "elif TYPE_OF_SEGMENTATION == \"ALL\":\n",
    "    preds[preds == 3] = 4\n",
    "else:\n",
    "    raise ValueError('invalid segmentation type.')\n",
    "\n",
    "for b in range(preds.shape[0]):\n",
    "    if np.count_nonzero(preds[b, :, :, :]) == 0:\n",
    "        print(\"all 0\")\n",
    "        preds[b, 0, 0, 0] = 1 # to avoid invalid data errors when submitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabulous-spray",
   "metadata": {},
   "outputs": [],
   "source": [
    "PREDS_DIR = \"data/predictions/\" \n",
    "path = PREDS_DIR\n",
    "samples = sorted(glob.glob('{}/*/'.format(TRAINING_DATA_ROOT_DIR + VAL_DIRS[0])))\n",
    "#samples1 = sorted(glob.glob('{}/*/'.format(TRAINING_DATA_ROOT_DIR + TRAIN_DIRS[0])))\n",
    "#samples2 = sorted(glob.glob('{}/*/'.format(TRAINING_DATA_ROOT_DIR + TRAIN_DIRS[1])))\n",
    "#samples = samples1 + samples2\n",
    "#samples = sorted(samples)\n",
    "samples = [x.split('/')[-2] for x in samples ]\n",
    "\n",
    "\n",
    "os.makedirs(path, exist_ok=True)\n",
    "\n",
    "print(samples)\n",
    "\n",
    "for i in range(preds.shape[0]):\n",
    "    s = samples[i]\n",
    "    output = preds[i, :, :, :]\n",
    "    filename = os.path.join(path, \"{}.nii.gz\".format(s))\n",
    "    ni_img = nib.Nifti1Image(output, affine=np.eye(4))\n",
    "    nib.save(ni_img, filename)\n",
    "    print(s, np.count_nonzero(preds[i, :, :, :]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gothic-counter",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-7.m65",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-7:m65"
  },
  "kernelspec": {
   "display_name": "machine_learning",
   "language": "python",
   "name": "machine_learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
