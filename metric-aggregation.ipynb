{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb905a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "from IPython.display import display\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "import dataframe_image as dfi\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "TYPE_OF_SEGMENTATION = \"TC\"\n",
    "\n",
    "RESULTS_FOLDER = f\"results/{TYPE_OF_SEGMENTATION.lower()}\"\n",
    "\n",
    "DATA_FILE = os.path.join(RESULTS_FOLDER, f\"{TYPE_OF_SEGMENTATION}_results.json\")\n",
    "\n",
    "os.makedirs(RESULTS_FOLDER, exist_ok=True)\n",
    "\n",
    "\n",
    "\n",
    "with open(DATA_FILE, \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "    \n",
    "def compute_mean(samples, dim):\n",
    "    ret = 0\n",
    "    for s in samples:\n",
    "        ret += s\n",
    "    \n",
    "    ret /= len(samples)\n",
    "    \n",
    "    return ret\n",
    "    \n",
    "def compute_margin(samples):\n",
    "    vals = [s for s in samples]\n",
    "    t_val = stats.t.ppf(q=0.975, df=len(samples)-1)\n",
    "    \n",
    "    stddev = np.std(vals)\n",
    "    \n",
    "    sig = stddev/np.sqrt(len(samples))\n",
    "    return t_val * sig\n",
    "\n",
    "\n",
    "dfs = {}\n",
    "\n",
    "with pd.ExcelWriter(os.path.join(RESULTS_FOLDER, f\"{TYPE_OF_SEGMENTATION}_results.xlsx\")) as writer:  \n",
    "    for dim in [\"mean\", \"median\"]:\n",
    "        for seqs, ckpts in data.items():\n",
    "            seq_df = {}\n",
    "            for ckpt, folds in ckpts.items():\n",
    "                l = []\n",
    "                for i in range(5):\n",
    "                    l.append(folds['fold-' + str(i + 1) + '-holdout'][dim])\n",
    "                    l[-1] = round(l[-1], 5)\n",
    "                    #l[-1]['median'] = round(l[-1]['median'], 5)\n",
    "\n",
    "                l_ext = []\n",
    "\n",
    "                mean = compute_mean(l, dim)\n",
    "\n",
    "                ci_margin = compute_margin(l)\n",
    "\n",
    "                l_ext.extend([mean, ci_margin, max(mean - ci_margin, 0), min(mean + ci_margin, 1)])\n",
    "\n",
    "                l.extend(l_ext)\n",
    "\n",
    "                seq_df[int(ckpt.split(\"_\")[-1])] = l\n",
    "\n",
    "            df = pd.DataFrame.from_dict(seq_df, orient='index', columns=['fold_' + str(i + 1) for i in range(5)] \n",
    "                            + [\"mean\", \"ci_margin\", \"ci_low\", \"ci_high\"])\n",
    "            name = TYPE_OF_SEGMENTATION + \"_\" + seqs + \"_\" + dim\n",
    "\n",
    "            df.index.name = name\n",
    "\n",
    "            display(df)\n",
    "\n",
    "            dfs[seqs + \"_\" + dim] = df\n",
    "\n",
    "            dfi.export(df, os.path.join(RESULTS_FOLDER, name + \".png\"), max_cols = -1)\n",
    "\n",
    "            df.to_excel(writer,\n",
    "                      sheet_name=f\"{TYPE_OF_SEGMENTATION}_{seqs}_{dim}\")\n",
    "        #         print(folds)\n",
    "        #         raise ValueError(\"S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5080c768",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_df = pd.DataFrame()\n",
    "median_df = pd.DataFrame()\n",
    "\n",
    "seqs = [\"t1ce\", \"flair\", \"t1ce,flair\", \"t1ce,flair,t1,t2\"]\n",
    "\n",
    "\n",
    "for seq in seqs:\n",
    "    for type1 in [\"mean\", \"median\"]:\n",
    "        name = seq + \"_\" + type1\n",
    "        df = dfs[name]\n",
    "    #     print(name)\n",
    "    #     print(df[\"mean\"])\n",
    "    #     df.plot()\n",
    "        name2 = name.split(\"_\")[0]\n",
    "        if \"median\" in name:\n",
    "            median_df[name2] = df[\"mean\"]\n",
    "        else:\n",
    "            mean_df[name2] = df[\"mean\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f0a923",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_df.columns = mean_df.columns.str.upper()\n",
    "median_df.columns = median_df.columns.str.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88d0acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "median_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613e77a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# et_median_t1ce = [0.27277, 0.82618, 0.82547, 0.84702, 0.84294, 0.86216, 0.85647, 0.82016, 0.85407, 0.85598, 0.8597, 0.83765]\n",
    "# et_median_flair = [0.19334, 0.18001, 0.24695, 0.25511, 0.28967, 0.19856, 0.24291, 0.22325, 0.19818, 0.25898, 0.14257, 0.13137]\n",
    "# et_median_t1ce_flair = [0.46656, 0.64605, 0.79329, 0.78737, 0.78554, 0.80364, 0.79944, 0.82448, 0.82426, 0.82091, 0.82694, 0.83706]\n",
    "# et_median_all = [0.44458, 0.64926, 0.61982, 0.73097, 0.76157, 0.75816, 0.67139, 0.6781, 0.58306, 0.79294, 0.76052, 0.77717 ]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# tc_median_t1ce = [0.64344, 0.85561, 0.87871, 0.87562, 0.87589, 0.88327, 0.86023, 0.87791, 0.88189, 0.897, 0.89414, 0.87359]\n",
    "# tc_median_flair = [0.47314, 0.51952, 0.542, 0.63161, 0.60847, 0.62028, 0.68299, 0.61957, 0.67037, 0.68205, 0.70904, 0.69833]\n",
    "# tc_median_t1ce_flair = [0.68196, 0.52993, 0.77157, 0.78406, 0.83826, 0.79296, 0.8626, 0.83409, 0.83638, 0.85976, 0.87472, 0.87369 ]\n",
    "# tc_median_all = [0.42093, 0.62322, 0.7209, 0.79264, 0.77772, 0.82913, 0.82071, 0.81572, 0.81129,  0.83061, 0.84407, 0.8391 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299e6292",
   "metadata": {},
   "outputs": [],
   "source": [
    "# median_df[\"T1CE\"] = et_median_t1ce\n",
    "# median_df[\"FLAIR\"] = et_median_flair\n",
    "# median_df[\"T1CE,FLAIR\"] = et_median_t1ce_flair\n",
    "# median_df[\"T1CE,FLAIR,T1,T2\"] = et_median_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f3ee2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# median_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c127ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # mean_df.index.name = f\"{TYPE_OF_SEGMENTATION} average of mean Dice Scores over k=5 fold CV\"\n",
    "# title = f\"{TYPE_OF_SEGMENTATION} mean of Dice Scores over k=5 fold CV (mean over samples)\"\n",
    "# # mean_df.plot()\n",
    "\n",
    "# mean_df_plot = mean_df.plot(xlabel=\"Epoch\", ylabel=\"Dice Score\", title=title, ylim=(0,1)).get_figure()\n",
    "# mean_df_plot.savefig(os.path.join(RESULTS_FOLDER, \"mean_df_plot.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f35dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "title = f\"{TYPE_OF_SEGMENTATION} validation results (median over samples)\"\n",
    "# median_df.plot()\n",
    "\n",
    "median_df_plot = median_df.plot(xlabel=\"Epoch\", ylabel=\"Dice Score\", title=title, ylim=(0,1)).get_figure()\n",
    "median_df_plot.savefig(os.path.join(RESULTS_FOLDER, \"median_df_plot.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a364dda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import statistics\n",
    "from math import sqrt\n",
    "\n",
    "\n",
    "def plot_confidence_interval(x, values, z=1.96, color='#2187bb', horizontal_line_width=0.25):\n",
    "\n",
    "    left = x - horizontal_line_width / 2\n",
    "    top = values[0] - values[1]\n",
    "    right = x + horizontal_line_width / 2\n",
    "    bottom = values[0] + values[1]\n",
    "    plt.plot([x, x], [top, bottom], color=color)\n",
    "    plt.plot([left, right], [top, top], color=color)\n",
    "    plt.plot([left, right], [bottom, bottom], color=color)\n",
    "    plt.plot(x, values[0], 'o', color='#f44336')\n",
    "\n",
    "    return mean\n",
    "\n",
    "\n",
    "#plt.xticks([1, 2, 3, 4], ['T1CE', 'FLAIR', 'T1CE,FLAIR', 'T1CE,FLAIR,T1,T2'])\n",
    "#plt.title('Confidence Interval (Mean)')\n",
    "# plot_confidence_interval(1, [10, 11, 42, 45, 44])\n",
    "# plot_confidence_interval(2, [10, 21, 42, 45, 44])\n",
    "# plot_confidence_interval(3, [20, 2, 4, 45, 44])\n",
    "# plot_confidence_interval(4, [30, 31, 42, 45, 44])\n",
    "# plt.show()\n",
    "\n",
    "CI_mean = {}\n",
    "CI_median = {}\n",
    "\n",
    "for name, df in dfs.items():\n",
    "#     print(name)\n",
    "#     print(df[\"mean\"])\n",
    "#     df.plot()\n",
    "    ckpt_60 = df.iloc[-1]\n",
    "    name2 = name.split(\"_\")[0]\n",
    "    if \"median\" in name:\n",
    "        CI_median[name2] = [ckpt_60[\"mean\"], ckpt_60[\"ci_margin\"]]\n",
    "        #median_df[name] = df[\"mean\"]\n",
    "    else:\n",
    "        CI_mean[name2] = [ckpt_60[\"mean\"], ckpt_60[\"ci_margin\"]]\n",
    "        \n",
    "print(CI_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca751948",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xticks([1, 2, 3, 4], ['t1ce', 'flair', 't1ce,flair', 't1ce,flair,t1,t2'])\n",
    "plt.xlabel(\"Sequence sets\")\n",
    "plt.ylabel(\"Dice score\")\n",
    "plt.title(f\"{TYPE_OF_SEGMENTATION} Confidence Interval (Mean) 60 epochs\")\n",
    "plot_confidence_interval(1, CI_mean[\"t1ce\"])\n",
    "plot_confidence_interval(2, CI_mean[\"flair\"])\n",
    "plot_confidence_interval(3, CI_mean[\"t1ce,flair\"])\n",
    "plot_confidence_interval(4, CI_mean[\"t1ce,flair,t1,t2\"])\n",
    "# plt.show()\n",
    "plt.savefig(os.path.join(RESULTS_FOLDER, \"ci_mean.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3377518e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xticks([1, 2, 3, 4], ['t1ce', 'flair', 't1ce,flair', 't1ce,flair,t1,t2'])\n",
    "plt.xlabel(\"Sequence sets\")\n",
    "plt.ylabel(\"Dice score\")\n",
    "plt.title(f\"{TYPE_OF_SEGMENTATION} Confidence Interval (Median) 60 epochs\")\n",
    "plot_confidence_interval(1, CI_median[\"t1ce\"])\n",
    "plot_confidence_interval(2, CI_median[\"flair\"])\n",
    "plot_confidence_interval(3, CI_median[\"t1ce,flair\"])\n",
    "plot_confidence_interval(4, CI_median[\"t1ce,flair,t1,t2\"])\n",
    "# plt.show()\n",
    "plt.savefig(os.path.join(RESULTS_FOLDER, \"ci_median.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08377cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://thedatascientist.com/how-to-do-a-t-test-in-python/\n",
    "from numpy.random import seed\n",
    "from numpy.random import randn\n",
    "from numpy.random import normal\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "seqs = [\"t1ce\", \"flair\", \"t1ce,flair\", \"t1ce,flair,t1,t2\"]\n",
    "\n",
    "result_df_equal = pd.DataFrame(columns=[\"comparison\", \"samples_A\", \"samples_B\", \"t-statistic\", \"p-value\", \"Reject null hypothesis\"])\n",
    "result_df_greater = pd.DataFrame(columns=[\"comparison\", \"samples_A\", \"samples_B\", \"t-statistic\", \"p-value\", \"Reject null hypothesis\"])\n",
    "\n",
    "\n",
    "def get_data(seq, epoch, scoreType):\n",
    "    seq_name = seq + \"_\" + scoreType\n",
    "    seq_df = dfs[seq_name]\n",
    "    if epoch == 60:\n",
    "        seq_df_epoch = seq_df.iloc[-1]\n",
    "    else:\n",
    "        seq_df_epoch = seq_df.iloc[-5]\n",
    "    seq_samples = [seq_df_epoch[f\"fold_{i}\"] for i in range(1, 6)]\n",
    "    \n",
    "    return seq_name, seq_samples, seq_df_epoch[\"mean\"]\n",
    "\n",
    "\n",
    "def do_ttest(seqA, seqB, epoch=60, scoreType=\"median\", equal_var=False):\n",
    "    global result_df_equal\n",
    "    global result_df_greater\n",
    "    print(f\"Doing t-test for {seqA} vs {seqB} at epoch {epoch} for {TYPE_OF_SEGMENTATION} segmentation and for metric type {scoreType}:\")\n",
    "    print(f\"Equal_var is set to {equal_var}\")\n",
    "\n",
    "    seqA_name, seqA_samples, seqA_epoch_mean = get_data(seqA, epoch, scoreType)\n",
    "    print(\"Statistics for\", scoreType)\n",
    "    print(seqA_name, seqA_samples, \"mean:\", seqA_epoch_mean)\n",
    "\n",
    "    print()\n",
    "\n",
    "    seqB_name, seqB_samples, seqB_epoch_mean = get_data(seqB, epoch, scoreType)\n",
    "    print(seqB_name, seqB_samples, \"mean:\", seqB_epoch_mean)\n",
    "\n",
    "    print(\"######################\")\n",
    "#     print(f\"Comparing {seqA_name} vs {seqB_name} for {TYPE_OF_SEGMENTATION} segmentation:\")\n",
    "    print(\"Alternative: Two-sided\")\n",
    "    t, p = ttest_ind(seqA_samples, seqB_samples, equal_var=equal_var)\n",
    "    insert_data = {\n",
    "        \"comparison\": f\"{seqA} vs {seqB}: epoch={epoch} equal_var={equal_var}\",\n",
    "        \"samples_A\": seqA_samples,\n",
    "        \"samples_B\": seqB_samples,\n",
    "        \"t-statistic\": t,\n",
    "        \"p-value\": p,\n",
    "        \"Reject null hypothesis\": str(p <= 0.05)\n",
    "    }\n",
    "    result_df_equal = pd.concat([result_df_equal, pd.DataFrame([insert_data])],axis=0, ignore_index=True)\n",
    "    print(f\"T-statistic: {t}, p-value: {p}\")\n",
    "    print(f\"p-value <= 0.05: {p <= 0.05}\")\n",
    "    print(\"Alternative: Less\")\n",
    "    t, p = ttest_ind(seqA_samples, seqB_samples, equal_var=equal_var, alternative='less')\n",
    "    insert_data = {\n",
    "        \"comparison\": f\"{seqA} vs {seqB}: epoch={epoch} assume equal_var={equal_var}\",\n",
    "        \"samples_A\": seqA_samples,\n",
    "        \"samples_B\": seqB_samples,\n",
    "        \"t-statistic\": t,\n",
    "        \"p-value\": p,\n",
    "        \"Reject null hypothesis\": str(p <= 0.05)\n",
    "    }\n",
    "    result_df_greater = pd.concat([result_df_greater, pd.DataFrame([insert_data])],axis=0, ignore_index=True)\n",
    "    print(f\"T-statistic: {t}, p-value: {p}\")\n",
    "    print(f\"p-value <= 0.05: {p <= 0.05}\")\n",
    "    print(\"######################\")\n",
    "    print()\n",
    "    print()\n",
    "    print()\n",
    "\n",
    "for eq in [True, False]:\n",
    "    do_ttest(\"t1ce\", \"t1ce,flair,t1,t2\", equal_var=eq)\n",
    "    do_ttest(\"t1ce,flair\", \"t1ce,flair,t1,t2\", equal_var=eq)\n",
    "    do_ttest(\"t1ce\", \"t1ce,flair\", equal_var=eq)\n",
    "    do_ttest(\"t1ce\", \"flair\", equal_var=eq)\n",
    "    if TYPE_OF_SEGMENTATION == \"TC\":\n",
    "        do_ttest(\"t1ce\", \"t1ce,flair,t1,t2\", epoch=40, equal_var=eq)\n",
    "        do_ttest(\"t1ce,flair\", \"t1ce,flair,t1,t2\", epoch=40, equal_var=eq)\n",
    "        do_ttest(\"t1ce\", \"t1ce,flair\", epoch=40, equal_var=eq)\n",
    "        do_ttest(\"t1ce\", \"flair\", epoch=40, equal_var=eq)\n",
    "\n",
    "\n",
    "\n",
    "result_df_equal = result_df_equal.style.set_caption(f\"{TYPE_OF_SEGMENTATION} segmentation (median scores) null hypothesis: equal means\")\n",
    "result_df_greater = result_df_greater.style.set_caption(f\"{TYPE_OF_SEGMENTATION} segmentation (median scores) null hypothesis: greater means\")\n",
    "\n",
    "        \n",
    "# result_df_equal\n",
    "\n",
    "# result_df_greater"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d7acde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataframe_image as dfi\n",
    "\n",
    "file_name = f\"{TYPE_OF_SEGMENTATION}__median-scores_t-test_null_hyp_eq.png\"\n",
    "\n",
    "dfi.export(result_df_equal, os.path.join(RESULTS_FOLDER, file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0605308c",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = f\"{TYPE_OF_SEGMENTATION}__median-scores_t-test_null_hyp_gt.png\"\n",
    "\n",
    "dfi.export(result_df_greater, os.path.join(RESULTS_FOLDER, file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc97618",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import f_oneway, kruskal\n",
    "\n",
    "\n",
    "epoch = 60\n",
    "\n",
    "print(f\"Running one-way parametric ANOVA for {TYPE_OF_SEGMENTATION}: assuming variances are not equal\")\n",
    "print(\"Classes are: t1ce, t1ce + flair, t1ce + flair + t1 + t2\")\n",
    "print(f\"Epoch is {epoch} with median scores used\")\n",
    "\n",
    "t1ce_name, t1ce_samples, t1ce_mean = get_data(\"t1ce\", epoch, \"median\")\n",
    "t1ce_flair_name, t1ce_flair_samples, t1ce_flair_mean = get_data(\"t1ce,flair\", epoch, \"median\")\n",
    "all_name, all_samples, all_mean = get_data(\"t1ce,flair,t1,t2\", epoch, \"median\")\n",
    "\n",
    "print(\"t1ce: \", t1ce_name, t1ce_samples, t1ce_mean)\n",
    "print(\"t1ce,flair: \", t1ce_flair_name, t1ce_flair_samples, t1ce_flair_mean)\n",
    "print(\"t1ce,flair,t1,t2: \", all_name, all_samples, all_mean)\n",
    "\n",
    "F, p = kruskal(t1ce_samples, t1ce_flair_samples, all_samples)\n",
    "\n",
    "print(f\"F-statistic is {F}, p-value is {p}\")\n",
    "print(f\"p-value <= 0.05 (Reject null hypothesis): {p <= 0.05}\")\n",
    "\n",
    "\n",
    "print()\n",
    "print()\n",
    "print()\n",
    "print()\n",
    "\n",
    "\n",
    "print(f\"Running one-way parametric ANOVA for {TYPE_OF_SEGMENTATION}: assuming variances are equal\")\n",
    "print(\"Classes are: t1ce, t1ce + flair, t1ce + flair + t1 + t2\")\n",
    "print(f\"Epoch is {epoch} with median scores used\")\n",
    "\n",
    "t1ce_name, t1ce_samples, t1ce_mean = get_data(\"t1ce\", epoch, \"median\")\n",
    "t1ce_flair_name, t1ce_flair_samples, t1ce_flair_mean = get_data(\"t1ce,flair\", epoch, \"median\")\n",
    "all_name, all_samples, all_mean = get_data(\"t1ce,flair,t1,t2\", epoch, \"median\")\n",
    "\n",
    "print(\"t1ce: \", t1ce_name, t1ce_samples, t1ce_mean)\n",
    "print(\"t1ce,flair: \", t1ce_flair_name, t1ce_flair_samples, t1ce_flair_mean)\n",
    "print(\"t1ce,flair,t1,t2: \", all_name, all_samples, all_mean)\n",
    "\n",
    "F, p = f_oneway(t1ce_samples, t1ce_flair_samples, all_samples)\n",
    "\n",
    "print(f\"F-statistic is {F}, p-value is {p}\")\n",
    "print(f\"p-value <= 0.05 (Reject null hypothesis): {p <= 0.05}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a3e4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 60\n",
    "\n",
    "print(f\"Running one-way parametric ANOVA for {TYPE_OF_SEGMENTATION}: assuming variances are not equal\")\n",
    "print(\"Classes are: t1ce, flair, t1ce + flair, t1ce + flair + t1 + t2\")\n",
    "print(f\"Epoch is {epoch} with median scores used\")\n",
    "\n",
    "t1ce_name, t1ce_samples, t1ce_mean = get_data(\"t1ce\", epoch, \"median\")\n",
    "flair_name, flair_samples, flair_mean = get_data(\"flair\", epoch, \"median\")\n",
    "t1ce_flair_name, t1ce_flair_samples, t1ce_flair_mean = get_data(\"t1ce,flair\", epoch, \"median\")\n",
    "all_name, all_samples, all_mean = get_data(\"t1ce,flair,t1,t2\", epoch, \"median\")\n",
    "\n",
    "print(\"t1ce: \", t1ce_name, t1ce_samples, t1ce_mean)\n",
    "print(\"flair: \", flair_name, flair_samples, flair_mean)\n",
    "print(\"t1ce,flair: \", t1ce_flair_name, t1ce_flair_samples, t1ce_flair_mean)\n",
    "print(\"t1ce,flair,t1,t2: \", all_name, all_samples, all_mean)\n",
    "\n",
    "F, p = kruskal(t1ce_samples, flair_samples, t1ce_flair_samples, all_samples)\n",
    "\n",
    "print(f\"F-statistic is {F}, p-value is {p}\")\n",
    "print(f\"p-value <= 0.05 (Reject null hypothesis): {p <= 0.05}\")\n",
    "\n",
    "\n",
    "print()\n",
    "print()\n",
    "print()\n",
    "print()\n",
    "\n",
    "\n",
    "print(f\"Running one-way parametric ANOVA for {TYPE_OF_SEGMENTATION}: assuming variances are equal\")\n",
    "print(\"Classes are: t1ce, t1ce + flair, t1ce + flair + t1 + t2\")\n",
    "print(f\"Epoch is {epoch} with median scores used\")\n",
    "\n",
    "t1ce_name, t1ce_samples, t1ce_mean = get_data(\"t1ce\", epoch, \"median\")\n",
    "flair_name, flair_samples, flair_mean = get_data(\"flair\", epoch, \"median\")\n",
    "t1ce_flair_name, t1ce_flair_samples, t1ce_flair_mean = get_data(\"t1ce,flair\", epoch, \"median\")\n",
    "all_name, all_samples, all_mean = get_data(\"t1ce,flair,t1,t2\", epoch, \"median\")\n",
    "\n",
    "print(\"t1ce: \", t1ce_name, t1ce_samples, t1ce_mean)\n",
    "print(\"t1ce: \", flair_name, flair_samples, flair_mean)\n",
    "print(\"t1ce,flair: \", t1ce_flair_name, t1ce_flair_samples, t1ce_flair_mean)\n",
    "print(\"t1ce,flair,t1,t2: \", all_name, all_samples, all_mean)\n",
    "\n",
    "F, p = f_oneway(t1ce_samples, flair_samples, t1ce_flair_samples, all_samples)\n",
    "\n",
    "print(f\"F-statistic is {F}, p-value is {p}\")\n",
    "print(f\"p-value <= 0.05 (Reject null hypothesis): {p <= 0.05}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d46739",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine_learning",
   "language": "python",
   "name": "machine_learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
