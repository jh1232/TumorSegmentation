{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "heavy-blowing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports and constants\n",
    "import os\n",
    "import glob\n",
    "import nibabel as nib\n",
    "import nilearn\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import scipy\n",
    "import shutil\n",
    "from tensorflow import keras\n",
    "from keras import Sequential, Model\n",
    "from keras.layers import Dense, Flatten, Dropout, Reshape, Conv3D, Conv2D, Activation, UpSampling3D, \\\n",
    "                         MaxPooling3D, SpatialDropout3D, BatchNormalization, Conv3DTranspose\n",
    "from keras.layers import concatenate, Input\n",
    "# from keras.engine import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nilearn import image\n",
    "from datetime import datetime\n",
    "from scipy import ndimage\n",
    "import keras_unet\n",
    "import imageio\n",
    "\n",
    "\n",
    "###### MODIFY HPARAMS ###########\n",
    "\n",
    "DEBUG = True # for testing purposes\n",
    "\n",
    "K_FOLD_VAL_NUM = \"ALLFOLDS\"\n",
    "\n",
    "SEQUENCES = [\"t1ce\", \"flair\", \"t1\", \"t2\"]\n",
    "\n",
    "TYPE_OF_SEGMENTATION = \"ET\"\n",
    "\n",
    "##################################\n",
    "\n",
    "\n",
    "N_EPOCHS = 60 if DEBUG == False else 2\n",
    "\n",
    "\n",
    "DEBUG_SAMPLES = 20\n",
    "\n",
    "print(tf.test.is_gpu_available())\n",
    "\n",
    "# TRAINING_DATA_ROOT_DIR = \"data/\"\n",
    "# TRAIN_DIRS = [\"train/LGG\", \"train/HGG\"]\n",
    "\n",
    "TRAINING_DATA_ROOT_DIR = \"data_folds/\"\n",
    "\n",
    "TRAIN_DIRS = [\"fold_\" + str(i + 1) for i in range(5) if i + 1 != K_FOLD_VAL_NUM]\n",
    "\n",
    "print(TRAIN_DIRS)\n",
    "\n",
    "VAL_DIRS = [\"val\"]\n",
    "\n",
    "\n",
    "GROUND_TRUTH_FILE = \"seg\"\n",
    "\n",
    "\n",
    "\n",
    "N_X = 240\n",
    "N_Y = 240\n",
    "N_Z = 155\n",
    "NUM_CHANNELS = len(SEQUENCES)\n",
    "NUM_CLASSES = 4 if TYPE_OF_SEGMENTATION == \"ALL\" else 2\n",
    "\n",
    "\n",
    "PATCH_X = 80\n",
    "PATCH_Y = 80\n",
    "PATCH_Z = 80\n",
    "\n",
    "\n",
    "BATCH_SIZE = 2\n",
    "\n",
    "PERCENTILE_95 = 10.204082 * 2 # 20 pixels\n",
    "\n",
    "SEQUENCES_JOINED = \",\".join(SEQUENCES)\n",
    "\n",
    "EXPERIMENT_NAME = f\"{TYPE_OF_SEGMENTATION}_{SEQUENCES_JOINED}_{N_EPOCHS}-epochs_fold-{K_FOLD_VAL_NUM}-holdout\"\n",
    "\n",
    "if DEBUG:\n",
    "    EXPERIMENT_NAME = \"DEBUG_EXPERIMENT\"\n",
    "\n",
    "\n",
    "print(EXPERIMENT_NAME)\n",
    "\n",
    "EXPERIMENT_FOLDER = os.path.join(\"experiments\", EXPERIMENT_NAME)\n",
    "\n",
    "CKPT_PATH_PREFIX = os.path.join(EXPERIMENT_FOLDER, EXPERIMENT_NAME + \"_ckpt\")\n",
    "\n",
    "print(CKPT_PATH_PREFIX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08059ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import tensorflow.keras\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "import tensorflow as tf\n",
    "\n",
    "print(f\"Tensor Flow Version: {tf.__version__}\")\n",
    "print(f\"Keras Version: {tensorflow.keras.__version__}\")\n",
    "print()\n",
    "print(f\"Python {sys.version}\")\n",
    "print(f\"Pandas {pd.__version__}\")\n",
    "print(f\"Scikit-Learn {sk.__version__}\")\n",
    "gpu = len(tf.config.list_physical_devices('GPU'))>0\n",
    "print(\"GPU is\", \"available\" if gpu else \"NOT AVAILABLE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scenic-stomach",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing\n",
    "\n",
    "def _standardize_nii_file(x):\n",
    "    ret = np.copy(x)\n",
    "    ret = (ret - x.mean())/x.std() if x.std() > 0 else (ret - x.mean())\n",
    "    return ret\n",
    "\n",
    "def standardize_training_features(X, y):\n",
    "    for i in range(X.shape[0]):\n",
    "        X[i, :, :, :, :] = np.array(_standardize_nii_file(X[i, :, :, :, :]))\n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "perfect-seattle",
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics\n",
    "\n",
    "def recall(y, y_hat):\n",
    "    true_positives = K.sum(K.round(K.clip(y * y_hat, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y, 0, 1)))\n",
    "    return true_positives/(possible_positives + K.epsilon())\n",
    "\n",
    "def precision(y, y_hat):\n",
    "    true_positives = K.sum(K.round(K.clip(y * y_hat, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_hat, 0, 1)))\n",
    "    return true_positives/(predicted_positives + K.epsilon())\n",
    "\n",
    "def f1(y, y_hat):\n",
    "    r = recall(y, y_hat)\n",
    "    p = precision(y, y_hat)\n",
    "    return 2*((p*r)/(p + r + K.epsilon()))\n",
    "\n",
    "\n",
    "def dice_coef(y_true, y_pred, smooth=1):\n",
    "    intersection = K.sum(K.abs(y_true * y_pred), axis=(0, 1, 2, 3))\n",
    "    dice_numerator = (2. * intersection + smooth)\n",
    "    dice_denominator = (K.sum(K.square(y_true),axis=(0, 1, 2, 3)) + K.sum(K.square(y_pred),axis=(0, 1, 2, 3)) + smooth)\n",
    "\n",
    "    return dice_numerator/dice_denominator\n",
    "\n",
    "\n",
    "def dice_coef_loss(y_true, y_pred):\n",
    "    return 1- K.mean(dice_coef(y_true, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70b34fd",
   "metadata": {},
   "source": [
    "# Statistics Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc8bdd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#validation inference\n",
    "\n",
    "CHECKPOINTS = {\n",
    "#     \"TC_flair_60-epochs_fold-1-holdout\": \"TC\",\n",
    "#     \"TC_t1ce_60-epochs_fold-1-holdout\": \"TC\",\n",
    "#     \"TC_flair_60-epochs_fold-2-holdout\": \"NotNeeded\",\n",
    "#     \"TC_flair_60-epochs_fold-3-holdout\": \"NotNeeded\",\n",
    "#     \"TC_flair_60-epochs_fold-4-holdout\": \"NotNeeded\",\n",
    "#     \"TC_flair_60-epochs_fold-5-holdout\": \"NotNeeded\",\n",
    "#     \"TC_t1ce_60-epochs_fold-2-holdout\": \"NotNeeded\",\n",
    "#     \"TC_t1ce_60-epochs_fold-3-holdout\": \"NotNeeded\",\n",
    "#     \"TC_t1ce_60-epochs_fold-4-holdout\": \"NotNeeded\",\n",
    "#     \"TC_t1ce_60-epochs_fold-5-holdout\": \"NotNeeded\",\n",
    "#     \"TC_t1ce,flair_60-epochs_fold-1-holdout\": \"NotNeeded\",\n",
    "#     \"TC_t1ce,flair_60-epochs_fold-2-holdout\": \"NotNeeded\",\n",
    "#     \"TC_t1ce,flair_60-epochs_fold-3-holdout\": \"NotNeeded\",\n",
    "#     \"TC_t1ce,flair_60-epochs_fold-4-holdout\": \"NotNeeded\",\n",
    "#     \"TC_t1ce,flair_60-epochs_fold-5-holdout\": \"NotNeeded\",\n",
    "#     \"TC_t1ce,flair,t1,t2_60-epochs_fold-1-holdout\": \"NotNeeded\",\n",
    "#     \"TC_t1ce,flair,t1,t2_60-epochs_fold-2-holdout\": \"NotNeeded\",\n",
    "#     \"TC_t1ce,flair,t1,t2_60-epochs_fold-3-holdout\": \"NotNeeded\",\n",
    "#     \"TC_t1ce,flair,t1,t2_60-epochs_fold-4-holdout\": \"NotNeeded\",\n",
    "#     \"TC_t1ce,flair,t1,t2_60-epochs_fold-5-holdout\": \"NotNeeded\",\n",
    "#     \"ET_flair_60-epochs_fold-1-holdout\": \"NotNeeded\",\n",
    "#     \"ET_t1ce_60-epochs_fold-1-holdout\": \"NotNeeded\",\n",
    "#     \"ET_flair_60-epochs_fold-2-holdout\": \"NotNeeded\",\n",
    "#     \"ET_flair_60-epochs_fold-3-holdout\": \"NotNeeded\",\n",
    "#     \"ET_flair_60-epochs_fold-4-holdout\": \"NotNeeded\",\n",
    "#     \"ET_flair_60-epochs_fold-5-holdout\": \"NotNeeded\",\n",
    "#     \"ET_t1ce_60-epochs_fold-2-holdout\": \"NotNeeded\",\n",
    "#     \"ET_t1ce_60-epochs_fold-3-holdout\": \"NotNeeded\",\n",
    "#     \"ET_t1ce_60-epochs_fold-4-holdout\": \"NotNeeded\",\n",
    "#     \"ET_t1ce_60-epochs_fold-5-holdout\": \"NotNeeded\",\n",
    "#     \"ET_t1ce,flair,t1,t2_60-epochs_fold-1-holdout\": \"NotNeeded\",\n",
    "#     \"ET_t1ce,flair,t1,t2_60-epochs_fold-2-holdout\": \"NotNeeded\",\n",
    "#     \"ET_t1ce,flair,t1,t2_60-epochs_fold-3-holdout\": \"NotNeeded\",\n",
    "#     \"ET_t1ce,flair,t1,t2_60-epochs_fold-4-holdout\": \"NotNeeded\",\n",
    "#     \"ET_t1ce,flair,t1,t2_60-epochs_fold-5-holdout\": \"NotNeeded\",\n",
    "#     \"ET_t1ce,flair_60-epochs_fold-1-holdout\": \"NotNeeded\",\n",
    "#     \"ET_t1ce,flair_60-epochs_fold-2-holdout\": \"NotNeeded\",\n",
    "#     \"ET_t1ce,flair_60-epochs_fold-3-holdout\": \"NotNeeded\",\n",
    "#     \"ET_t1ce,flair_60-epochs_fold-4-holdout\": \"NotNeeded\",\n",
    "#     \"ET_t1ce,flair_60-epochs_fold-5-holdout\": \"NotNeeded\",\n",
    "      \"ET_t1ce,flair,t1,t2_60-epochs_fold-ALLFOLDS-holdout\": \"NotNeeded\",\n",
    "}\n",
    "\n",
    "# SEQUENCES_LIST = {\n",
    "#     \"TC_t1ce_60-epochs_fold-1-holdout\": [\"t1ce\"],\n",
    "#     \"TC_flair_60-epochs_fold-1-holdout\": [\"flair\"],\n",
    "# }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def read_validation_data(root_path, dirs, seq_list):\n",
    "    # list of numpy arrays\n",
    "    X = []\n",
    "    sample_names = []\n",
    "    \n",
    "    for d in dirs:\n",
    "        path = root_path + d\n",
    "        samples = sorted(glob.glob('{}/*/'.format(path)))\n",
    "\n",
    "        for t in samples:\n",
    "            x_sample = []\n",
    "            y_sample = []\n",
    "            for s in seq_list:\n",
    "                x_file = glob.glob('{}/*{}.nii.gz'.format(t, s))[0] # this is unique\n",
    "                x_sample.append(np.array(nilearn.image.get_data(x_file)))\n",
    "                \n",
    "            X.append(np.array(x_sample))\n",
    "            sample_names.append(t)\n",
    "    X = np.array(X)\n",
    "    print(X.shape)\n",
    "    X = np.moveaxis(X, [0, 1, 2, 3, 4], [0, 4, 1, 2, 3])\n",
    "    print(X.shape)\n",
    "    return sample_names, X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d41e7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "root_folder = \"experiments\"\n",
    "\n",
    "def do_pred_with_ckpt(exp_name, c, segType, seq_list, train_folders):\n",
    "    model = keras.models.load_model(c, custom_objects={'dice_coef_loss': dice_coef_loss, 'f1': f1})\n",
    "    X = None\n",
    "    y = None\n",
    "#     X_validation = read_validation_data(TRAINING_DATA_ROOT_DIR, VAL_DIRS, seq_list)\n",
    "    \n",
    "    if train_folders != [\"val\"]:\n",
    "        samples, X_validation = read_validation_data(TRAINING_DATA_ROOT_DIR, train_folders, seq_list)\n",
    "    else:\n",
    "        samples, X_validation = read_validation_data(\"data/\", [\"val/val\"], seq_list)\n",
    "\n",
    "    print(\"X_validation shape: \", X_validation.shape)\n",
    "    #print(\"y shape: \", y.shape)\n",
    "    \n",
    "    NUM_CLASSES = 4 if segType == \"ALL\" else 2\n",
    "    \n",
    "    NUM_CHANNELS = len(seq_list)\n",
    "    \n",
    "    #print(X_validation.shape)\n",
    "    preds = np.zeros((X_validation.shape[0], X_validation.shape[1], X_validation.shape[2], X_validation.shape[3] + 5, NUM_CLASSES))\n",
    "\n",
    "    tmp111 = np.zeros((X_validation.shape[0], X_validation.shape[1], X_validation.shape[2], X_validation.shape[3] + 5, NUM_CHANNELS))\n",
    "\n",
    "    tmp111[:, :, :, :-5, :] = X_validation\n",
    "\n",
    "    X_validation = tmp111\n",
    "\n",
    "    #print(X_validation.shape)\n",
    "\n",
    "    for b in range(0, X_validation.shape[0]):\n",
    "        #print(b)\n",
    "        K.clear_session()\n",
    "        _ = gc.collect()\n",
    "        X_normalized, _ = standardize_training_features(np.copy(X_validation[b:b+1, :, :, :, :]), None)\n",
    "        logits = model.predict(X_normalized, batch_size=1, verbose=0)\n",
    "        logits = np.squeeze(logits)\n",
    "        preds[b, :, :, :, :] += logits\n",
    "\n",
    "\n",
    "    tmp111 = None\n",
    "    X_validation = None\n",
    "\n",
    "    #print(preds.shape)\n",
    "\n",
    "    preds = preds[:, :, :, :-5, :]\n",
    "\n",
    "    #print(preds.shape)\n",
    "\n",
    "    preds = np.argmax(preds, axis=-1)\n",
    "    \n",
    "    if segType == \"TC\":\n",
    "        pass\n",
    "    elif segType == \"ET\":\n",
    "        preds[preds == 1] = 4\n",
    "    elif segType == \"ALL\":\n",
    "        preds[preds == 3] = 4\n",
    "    else:\n",
    "        raise ValueError('invalid segmentation type.')\n",
    "\n",
    "    for b in range(preds.shape[0]):\n",
    "        if np.count_nonzero(preds[b, :, :, :]) == 0:\n",
    "            #print(\"all 0\")\n",
    "            preds[b, 0, 0, 0] = 1 # to avoid invalid data errors when submitting\n",
    "    \n",
    "    chkpt_folder_name = c[:-3].replace(\".\", \"_\").split(\"/\")[-1]\n",
    "    \n",
    "    if train_folders == [\"val\"]:\n",
    "        chkpt_folder_name += \"_val\"\n",
    "    \n",
    "    segmentations_subpath = \"segmentations_fixed\" if train_folders != [\"val\"] else \"val_segmentations\"\n",
    "    \n",
    "    #PREDS_DIR = \"loop_preds/predictions5/\" \n",
    "    path = os.path.join(root_folder, exp_name, segmentations_subpath, chkpt_folder_name)\n",
    "    \n",
    "    #samples = sorted(glob.glob('{}/*/'.format(TRAINING_DATA_ROOT_DIR + VAL_DIRS[0])))\n",
    "    #samples1 = sorted(glob.glob('{}/*/'.format(TRAINING_DATA_ROOT_DIR + TRAIN_DIRS[0])))\n",
    "    #samples2 = sorted(glob.glob('{}/*/'.format(TRAINING_DATA_ROOT_DIR + TRAIN_DIRS[1])))\n",
    "    #samples = samples1 + samples2\n",
    "    #samples = sorted(samples)\n",
    "    #samples = [x.split('/')[-2] for x in samples ]\n",
    "\n",
    "    #print(\"Outputting\", c, \"to folder\", path)\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "    #print(samples)\n",
    "\n",
    "    for i in range(preds.shape[0]):\n",
    "        s = samples[i]\n",
    "        s = s.split(\"/\")[-2]\n",
    "        output = preds[i, :, :, :]\n",
    "        output = output.astype(np.int32)\n",
    "        filename = os.path.join(path, \"{}.nii.gz\".format(s))\n",
    "        ni_img = nib.Nifti1Image(output, affine=np.eye(4))\n",
    "        nib.save(ni_img, filename)\n",
    "        #print(s, np.count_nonzero(preds[i, :, :, :]))\n",
    "\n",
    "\n",
    "\n",
    "def do_inference(f, segType):\n",
    "    chkpt_folder = os.path.join(root_folder, f)\n",
    "    print(chkpt_folder)\n",
    "\n",
    "    chkpts = [cp for cp in glob.glob(f\"{chkpt_folder}/*.h5\") if cp.startswith(chkpt_folder + \"/\" + f) and \"ckpt.h5\" not in cp]\n",
    "    #chkpts = [cp for cp in glob.glob(f\"{chkpt_folder}/*.h5\") if cp.startswith(chkpt_folder + \"/\" + f) and cp != chkpt_folder + \"/\" + f + \".h5\"]\n",
    "#     print(chkpts)\n",
    "    for c in chkpts:\n",
    "        print(c)\n",
    "#         TODO: filter out the fold used for holdout\n",
    "#         curr_folds = [\"fold_\" + str(i + 1) for i in range(5) if \"fold-\" + str(i+1) in f]\n",
    "        \n",
    "#         for fold in curr_folds:\n",
    "#             K.clear_session()\n",
    "#             _ = gc.collect()\n",
    "#             print(f, fold)\n",
    "#             do_pred_with_ckpt(f, c, segType, f.split(\"_\")[1].split(\",\"), [fold])\n",
    "        \n",
    "#         if \"ckpt.60.h5\" in c:\n",
    "        curr_folds = [\"val\"]\n",
    "        for fold in curr_folds:\n",
    "            K.clear_session()\n",
    "            _ = gc.collect()\n",
    "            print(f, fold)\n",
    "            do_pred_with_ckpt(f, c, segType, f.split(\"_\")[1].split(\",\"), [fold])\n",
    "\n",
    "\n",
    "for k, v in CHECKPOINTS.items():\n",
    "    segType = k.split(\"_\")[0]\n",
    "    do_inference(k, segType)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-7.m65",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-7:m65"
  },
  "kernelspec": {
   "display_name": "machine_learning",
   "language": "python",
   "name": "machine_learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
